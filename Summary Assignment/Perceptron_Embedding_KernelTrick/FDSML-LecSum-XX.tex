\documentclass[a4paper, 12pt]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}


%%% Meta data

	\title{Foundations of Data Science \& Machine Learning}
 
	\usepackage{authblk}
	\author{
		Summary | Week 02 \\
		Devansh Singh Rathore\\
		111701011
	}
	\affil{
			B.Tech. in Computer Science \& Engineering\\
		Indian Institute of Technology Palakkad
	}


%%% Page formatting

	
	\usepackage{hyperref}
	\hypersetup{colorlinks=false,linkcolor=red,citecolor=red,pdfborder={0 0 0}}

	\renewcommand{\arraystretch}{1.5}

%% Algorithms
	\usepackage{algorithm, algorithmic}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
% Math 
	\usepackage{amsmath,amsthm, amssymb}

	% Theorem environments
	\newtheorem{theorem}{Theorem}
	\newtheorem{corollary}[theorem]{Corollary}
	\newtheorem{lemma}[theorem]{Lemma}
	\newtheorem{proposition}[theorem]{Proposition}
	\newtheorem{conjecture}[theorem]{Conjecture}
	\newtheorem{observation}[theorem]{Conjecture}

	\theoremstyle{definition}
	\newtheorem{definition}[theorem]{Definition}
	\newtheorem{question}[theorem]{Question}

	\theoremstyle{remark}
	\newtheorem*{remark}{Remark}
	
	% Shorthands
	\def\bbN{\mathbb{N}}
	\def\bbZ{\mathbb{Z}}
	\def\bbQ{\mathbb{Q}}
	\def\bbR{\mathbb{R}}
	\def\bbF{\mathbb{F}}

	\def\tends{\rightarrow}
	\def\into{\rightarrow}
	\def\implies{\Rightarrow}
	\def\half{\frac{1}{2}}
	\def\quarter{\frac{1}{4}}
	
	\newcommand{\set}[1]{\left\{ #1 \right\}}
	\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
	\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}
	\newcommand{\card}[1]{\left\vert #1 \right\vert}
	\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
	\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}

\begin{document}
\maketitle

\begin{abstract}

We study about Perceptron Learning Algorithm and its proof of upper bound of its computational complexity. Further we discuss non-linear separators, embedding in higher dimensional space using Kernel trick.

\end{abstract}

\section{Perceptron learning algorithm}

$\rightarrow$ \textbf{Perceptron} is a supervised machine learning algorithm used for solving binary classification problems.

\vspace{0.5cm}
\textbf{NOTE: We assume b = 0. Equivalently there exists a separating hyperplane passing through origin.}

$x = (x_1, x_2, ...x_n) \in \bbR^n$, $b \in \bbR$

$x' = (x_1, x_2, ...x_n, 1) \in {\bbR}^{(n+1)}$

since $a = (a_1, a_2, ...a_n) \in \bbR^n$, assume $a' = (a_1, a_2, ...a_n, -b) \in \bbR^{(n+1)}$

For the good points we needed, $\ip{x}{a} \geq b + \delta / 2$ 

Now, $\ip{x'}{a'} = \ip{x}{a} - b \geq \delta / 2$

Similarly, for bad points we needed, $\ip{x}{a} \leq b - \delta / 2$
Now, $\ip{x'}{a'} = \ip{x}{a} - b \leq -\delta / 2$

\vspace{0.5cm}
\textbf{Input:} Two finite sets $G, B \subset \bbR^n$ which are linearly separable by a	hyperplane passing through the origin.

\vspace{0.5cm}
\textbf{Output:} $a \in \bbR^n$ such that 
	$\forall x \in G$, $\ip{x}{a} > 0$ and
	$\forall x \in B$, $\ip{x}{a} < 0$.

\vspace{0.5cm}
\textbf{Algorithm:} initially let's keep a = 0

Repeat until there is no update in a -

\hspace{0.5cm} for each $x \in G$ :

\hspace{1cm} if($\ip{x}{a} \leq 0$) :

\hspace{1.5cm} then a = a + x

\hspace{0.5cm} for each $x \in B$ :

\hspace{1cm} if($\ip{x}{a} \geq 0$) :

\hspace{1.5cm} then a = a - x

\vspace{0.5cm}
\textbf{Claim:} If G and B are linearly separable then the Perceptron algorithm terminates after $O(R^2 / \delta^2)$ i.e. $O(1 / {RelativeMargin^2})$ updates, where

R = $max_{x \in G \cup B} ||x||$

$\delta (margin) = min_{x \in G, y \in B} ||x - y||$

Relative Margin = $\delta / R$

\vspace{0.5cm}
\textbf{Proof:} Since G and B are given as linearly separable.
$\exists a^{*}$ s.t. $\forall x \in G$, $\ip{x}{a^{*}} \geq \delta / 2$ and $\forall x \in B$, $\ip{x}{a^{*}} \leq -\delta / 2$.

assume $||a^{*}|| = 1$, (w.l.o.g.)

$\rightarrow$ For $x \in G$, update occurs when $\ip{x}{a} \leq 0$

$a += x$

$\ip{a}{a^{*}} += \ip{x}{a^{*}}$

since $\ip{x}{a^{*}} \geq \delta / 2$, increase in $\ip{a}{a^{*}} \geq \delta / 2$

$||a||^2 = ||a + x||^2$, so $||a||^2$ increases by $2\ip{x}{a} + ||x||^2$ ($\ip{x}{a} \leq 0$)

i.e. $||a||^2$ increases by $\leq R^2$


$\rightarrow$ For $x \in B$, update occurs when $\ip{x}{a} \geq 0$

$a -= x$

$\ip{a}{a^{*}} -= \ip{x}{a^{*}}$

since $-\ip{x}{a^{*}} \geq \delta / 2$, increase in $\ip{a}{a^{*}} \geq \delta / 2$

$||a||^2 = ||a - x||^2$, so $||a||^2$ increases by $-2\ip{x}{a} + ||x||^2$ ($\ip{x}{a} \geq 0$)

i.e. $||a||^2$ increases by $\leq R^2$

$\rightarrow$ After k updates,

$||a|| \geq \ip{a}{a^{*}} \geq k.\delta / 2$, so $||a|| \geq k.\delta / 2$    \hspace{5cm}      ---(i)

while on the other hand $||a||^2 \leq k.R^2$ i.e. $||a|| \leq \sqrt{k}.R$      \hspace{2cm}     ---(ii)

From (i) and (ii), $k \leq 4 * R^2 / \delta^2$

\textbf{Hence there cannot be more than $4 * R^2 / \delta^2$ updates in Perceptron algorithm.}

\begin{figure}[!h]
\centering
 \includegraphics[width=0.90\textwidth]{3.0.png}
 
 Fig 0.0 Working of Perceptron Algorithm
 \end{figure}

$\rightarrow$ \textbf{Support Vector Machine (SVM)} is supervised machine learning algorithm used for optimal linear classification. SVM tries to find the maximum margin between Good and Bad points and finds suitable separating hyperplane.

\section{Non-linear Separators using the Kernel Trick}

\begin{figure}[!h]
\centering
 \includegraphics[width=0.40\textwidth]{lin_sep.png}
 \includegraphics[width=0.40\textwidth]{non_lin_sep.png}
 
 Fig 0.1 Linear Vs Non Linear Classification problem
 \end{figure}

$\rightarrow$ We can have multiple linear classifier chunks for classifying points which cannot be classified using a single linear classifier hyperplane.

$\rightarrow$ The base case of non-linear classification problem (Fig 0.1) is the XOR plot.

\begin{figure}[!h]
\centering
 \includegraphics[width=0.40\textwidth]{multi_NN.png}
 
 Fig 0.2 Multilayer Neural Network
 \end{figure}

\subsection{Embedding in a higher dimensional space}

$\rightarrow$ Let's take an example where the Good points are within a circular boundary in a 2D plane and the Bad points lie outside the circular margin of radius R.

Now, the \textbf{Embedding function} is defined as: $\phi : \bbR^n \rightarrow \bbR^N$.

Here in this example, $\phi : \bbR^2 \rightarrow \bbR^3$

$\phi((x_1, x_2)) = (x_1, x_2, x_1^2 + x_2^2)$

$\rightarrow$ Now in 3D space, the Good points lie below the plane $x3 (= x_1^2 + x_2^2) = R^2$, while the Bad points lie above this plane. Hence we can say that the $\phi$ images of G and B are classified using \textbf{linear separator in $\bbR^3$}.

$\rightarrow$ In general terms,

$\phi : \bbR \rightarrow \bbR^{N = d + 1}$ 

$\phi(x) = (1, x^1, x^2, ...x^d) \in \bbR^{d+1}$

Separating hyperplane in $\bbR^{d+1}$ looks like ($a_0, a_1, ...a_d$)

And for classification, we need to deal with polynomial equations like 

$a_0 + a_1x^1 + a_2x^2 + ...a_dx^d {>}_{<} 0$

$\rightarrow$ So you have at your disposal all polynomials of degree $\leq d$ as potential separators.
\begin{figure}[!h]
\centering
 \includegraphics[width=0.30\textwidth]{rn.png}
 
 Fig 0.3 Polynomial Classifier
 \end{figure}

$\rightarrow \phi : \bbR^2 \rightarrow \bbR^N$

for d = 3, $\phi((x_1, x_2)) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_1^3, x_2^3, x_1^2x_2, x_1x_2^2)$, so N = 10

$\rightarrow \phi : \bbR^3 \rightarrow \bbR^N$

for d = 2, n = 3, $\phi((x_1, x_2, x_3)) = (1, x_1, x_2, x_3, x_1^2, x_2^2, x_3^2, x_1x_2, x_1x_3, x_2x_3)$, so N = 10 

$\rightarrow$ In general, N(n, d) = number of terms with degree $\leq$ d in $(1 + x_1^1 + x_1^2 + ...x_1^d)(1 + x_2^1 + x_2^2 + ...x_2^d)...(1 + x_n^1 + x_n^2 + ...x_n^d)$

$= C^{n+d}_d$

= No. of monic monomials in n variables with degree $\leq$ d.

\vspace{0.5cm}
$\rightarrow$ \textbf{Perceptron Learning algorithm with embedding}

\textbf{Input:} $G = {x_1, x_2, ...x_k} \subseteq \bbR^n$

$B = {x_{k+1}, x_{k+2}, ...x_l} \subseteq \bbR^n$

$\phi: \bbR^n \rightarrow \bbR^N$

\textbf{Output:} $a \in \bbR^N$, $\forall x_j \in G, \ip{a}{\phi(x_j) > 0}$, $\forall x_j \in B, \ip{a}{\phi(x_j) < 0}$

\textbf{Algorithm:}

Initially $a = (0, 0, ...) \in \bbR^N$

Repeat until there is no update in a:

\hspace{0.5cm}
for each $x_j \in G$ :

\hspace{1cm}
if $\ip{a}{\phi(x_j)} < 0, a = a + \phi(x_j)$

\hspace{0.5cm}
for each $x_j \in B$ :

\hspace{1cm}
if $\ip{a}{\phi(x_j)} > 0, a = a - \phi(x_j)$

$\rightarrow$ This will terminate when $\phi_G = \{\phi(x): x \in G\}$ \& $\phi_B = \{\phi(x): x\in B\}$ are linearly separable in $\bbR^N$.

$\rightarrow$ This algorithm is computationally expensive when N is very large. We can make it efficient using the \textbf{"Kernel trick"}.


\subsection{Kernel Trick}
\hspace{0.45cm}
$\rightarrow$ Observation 1: Most of the computations are inner products of the form $\ip{a}{\phi(x)}$.

$\rightarrow$ Observation 2: a is a linear (integer) combination of $\phi(x_i), i \in G \cup B$ i.e. 
\[
a = \sum^l_{j=1}\alpha_j\phi(x_j)
\]

where $\alpha_j =$ isBad($x_j$) * number of times $x_j$ passed the update condition

isBad($x_j$) = -1 if $x_j \in B$, else 1.

$\rightarrow$ From Observation 1 \& 2 - 

$\ip{a}{\phi(x_i)} = \ip{\sum^l_{j=1}\alpha_j\phi(x_j)}{\phi(x_i)}$

\hspace{0.5cm}
$= \sum^l_{j=1}\alpha_j\ip{\phi(x_j)}{\phi(x_i)}$, where $x_i, x_j \in G \cup B$

\vspace{0.5cm}
$\rightarrow$ So the problem of computational efficiency boils down to checking whether we can  compute $\ip{\phi(x_i)}{\phi(x_j)}$ efficiently i.e. with complexity/time independent of N.

if $\phi: \bbR \rightarrow \bbR^{d+1}$

$\phi(x) = (1, x, x^2, ...x^d)$

$\phi(y) = (1, y, y^2, ...y^d)$

$\ip{\phi(x)}{\phi(y)} = 1 + xy + x^2y^2 + ... + x^dy^d$

\hspace{0.5cm}
$= (1 - (xy)^{d+1}) / ({1 - (xy)})$, using G.P.

\vspace{0.5cm}
$\rightarrow$ \textbf{Perceptron Learning algorithm with Kernel Trick}

\textbf{Input:} $G = {x_1, x_2, ...x_k} \subseteq \bbR^n$

$B = {x_{k+1}, x_{k+2}, ...x_l} \subseteq \bbR^n$

$K: \bbR^n x \bbR^n \rightarrow \bbR$, $K(x,y) = \ip{\phi(x)}{\phi(y)}$


\textbf{Output:} $(\alpha_1, \alpha_2, ...\alpha_l) \in \bbZ^l$, $\forall x_i \in G, \sum^l_{j=1}\alpha_j K(x_j, x_i) > 0$, $\forall x_i \in B, \sum^l_{j=1}\alpha_j K(x_j, x_i) < 0$

\textbf{Algorithm:}

Initially $\alpha = (0, 0, ...) \in \bbR^N$

Repeat until there is no update in $\alpha$:

\hspace{0.5cm}
for each $x_j \in G$ :

\hspace{1cm}
if $\sum^l_{i=1}\alpha_j K(x_j, x_i) \leq 0, \alpha_j += 1$

\hspace{0.5cm}
for each $x_j \in B$ :

\hspace{1cm}
if $\sum^l_{i=1}\alpha_j K(x_j, x_i) \geq 0, \alpha_j -= 1$




















\begin{comment}

\section{Nonlinear separators}

\subsection{Separation in a higher dimensional embedding}

\subsection{Kernel perceptron algorithm}

\begin{algorithm}
\caption{Kernel Perceptron Learning Algorithm}
\begin{algorithmic} 
\REQUIRE 
	A kernel function $K : (\bbR^n \times \bbR^n) \into \bbR$ and two finite
	subsets of $\bbR^n$ $G = \{x_1, \ldots, x_k\}$ and $B = \{x_{k+1}, \ldots,
	x_l\}$  which are  separable by $K$.
\ENSURE 
	$\alpha_1, \ldots \alpha_l \in \bbZ$ such that 
	\begin{align*}
		\forall x_j \in G,\quad & \sum_{i=1}^{l}\alpha_i K(x_i, x_j) > 0, \\
		\forall x_j \in B,\quad & \sum_{i=1}^{l}\alpha_i K(x_i, x_j) < 0.
	\end{align*}

\medskip
\STATE $\forall i ~ \alpha_i \leftarrow 0$
\REPEAT 
	\FORALL{$x \in G$}
	\STATE{...}
	\ENDFOR
	\FORALL{$x \in B$}
	\STATE{...}
	\ENDFOR
\UNTIL{no updates to $\alpha$}
\end{algorithmic}
\end{algorithm}

\end{comment}

\end{document}
