\documentclass[a4paper, 12pt]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}


%%% Meta data

	\title{Foundations of Data Science \& Machine Learning}
 
	\usepackage{authblk}
	\author{
		Summary | Week 03 \\
		Devansh Singh Rathore\\
		111701011
	}
	\affil{
			B.Tech. in Computer Science \& Engineering\\
		Indian Institute of Technology Palakkad
	}


%%% Page formatting

	
	\usepackage{hyperref}
	\hypersetup{colorlinks=false,linkcolor=red,citecolor=red,pdfborder={0 0 0}}

	\renewcommand{\arraystretch}{1.5}

%% Algorithms
	\usepackage{algorithm, algorithmic}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
% Math 
	\usepackage{amsmath,amsthm, amssymb}

	% Theorem environments
	\newtheorem{theorem}{Theorem}
	\newtheorem{corollary}[theorem]{Corollary}
	\newtheorem{lemma}[theorem]{Lemma}
	\newtheorem{proposition}[theorem]{Proposition}
	\newtheorem{conjecture}[theorem]{Conjecture}
	\newtheorem{observation}[theorem]{Conjecture}

	\theoremstyle{definition}
	\newtheorem{definition}[theorem]{Definition}
	\newtheorem{question}[theorem]{Question}

	\theoremstyle{remark}
	\newtheorem*{remark}{Remark}
	
	% Shorthands
	\def\bbN{\mathbb{N}}
	\def\bbZ{\mathbb{Z}}
	\def\bbQ{\mathbb{Q}}
	\def\bbR{\mathbb{R}}
	\def\bbF{\mathbb{F}}

	\def\tends{\rightarrow}
	\def\into{\rightarrow}
	\def\implies{\Rightarrow}
	\def\half{\frac{1}{2}}
	\def\quarter{\frac{1}{4}}
	
	\newcommand{\set}[1]{\left\{ #1 \right\}}
	\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
	\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}
	\newcommand{\card}[1]{\left\vert #1 \right\vert}
	\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
	\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}

\begin{document}
\maketitle

\begin{abstract}

We study about Support Vector Machine (SVM) and Maximum - Margin Separating Hyperplane (MMSHP) mathematically. Later, we look into generalisation of rule 'h' for future unknown points.

\end{abstract}

\section{Support Vector Machines (SVM)}

$\rightarrow$ Is one separating hyperplane better than other?

\begin{figure}[!h]
\centering
 \includegraphics[width=0.50\textwidth]{FDM_3.0.png}
 
 Fig 1.0 Separating hyperplanes
 \end{figure}

$\rightarrow$ In general, we try to find the hyperplane which is at the maximum distance from all the points i.e. $x \in G \cup B$.

$\rightarrow$ If a hyperplane equations depend on a and b where $a \in \bbR^n$, $b \in \bbR$ and $||a|| = 1$.

Then we have to $Maximise_{a,b}(Min_{x}|\ip{x}{a} - b|)$ among all the hyperplanes.

\subsection{Maximum - Margin Separating HyperPlane (MMSHP)}
$\rightarrow$ We also need to ensure that the hyperplane should be separating Good and Bad points i.e. lies between the $H_G$ and $H_B$.

\textbf{Definition:} For two linearly separable sets G, B $\in \bbR^n$, the \textbf{Maximum - Margin Separating HyperPlane (MMSHP)} is defined as $a \in \bbR^n$, $||a|| = 1$ and $b \in \bbR$ which maximises:
\[
    min_{x \in G \cup B} \, (\ip{x}{a} - b)f(x) 
\]    
\[
   where \, f(x) = \{^{ \, \, \, \, \, 1, \, x \in G}_{ \, -1, \, x \in B}
\]

$\rightarrow$ Perceptron algorithm finds one of the separating hyperplanes which need not be the MMSHP. While on the other hand, SVM finds MMSHP.

$\rightarrow$ Intutively, MMSHP is unique and separates the Good and Bad points in the most efficient manner. We will discuss this in later part of summary.

\vspace{0.5cm}
\textbf{Input:} G, B, where $X := G \cup B$ and $\delta: X \rightarrow \{1, -1\}$

\textbf{Objective:} $Maximise_{(a,b)}$ $Min_{x \in X} (\ip{x}{a} - b) f(x)$

\textbf{Constraint:} $||a|| = 1$

\vspace{0.5cm}
$\rightarrow$ Slight modification:

$\rightarrow$ Idea is to collect the (a,b) with \textbf{margin} ( = $Min_{x \in X} (\ip{x}{a} - b) f(x)$) $\geq 1$ and pick the tuple with minimum $||a||$.

\[
\forall x \in G, \, \, margin \, = \ip{x}{a} - b \geq 1
\]
\[
Dist(x, l) = \ip{x}{a/||a||} - b/||a|| \geq 1/||a||
\]
$\rightarrow$ So our new \textbf{Objective}: find line l which gives minimum $||a||^2$, to maximise Dist(x, l).

\textbf{Constraints:} 
\[
\forall x \in G, \, \, \ip{x}{a} - b \geq 1
\]
\[
\forall x \in B, \, \, \ip{x}{a} - b \leq -1
\]

$\rightarrow$ This is solved using \textbf{"Quadratic Programming"}.

\subsection{SVM with Embedding}

$\rightarrow \phi : \bbR^n \rightarrow V$

Minimise: $||a||^2$ among ($a \in V$)

Subject to: $(\ip{a}{\phi(x)} - b)f(x) \geq 1$

\begin{figure}[!h]
\centering
 \includegraphics[width=0.30\textwidth]{FDM_3.1.png}
 
 Fig 1.1 Embedding plot 
 \end{figure}
 
 $\bbR^n : V$
 
 $x_1, x_2, ...x_n : \phi(x_1), \phi(x_2), ...\phi(x_n)$
 
 $a \in \bbR^n$, $b \in \bbR : a \in V, \, b \in \bbR$

$K(x,y) = \ip{\phi(x)}{\phi(y)}$

\textbf{Key Obs.:} 'a' returned by SVM (a for the MMSHP) is linear combination of $\phi(dataVector)$ i.e. $a = \sum^{N}_{i=1}\alpha_i\phi(x_i)$.

$\ip{a}{\phi(x)} = \ip{\sum^{N}_{i=1}\alpha_i\phi(x_i)}{\phi(x)}$

\hspace{1.55cm} $= \sum^{N}_{i=1}\alpha_i \ip{\phi(x_i)}{\phi(x)}$

\hspace{1.55cm} $= \sum^{N}_{i=1}\alpha_i K(x_i, x_j)$

\vspace{0.5cm}
$||a||^2 = \ip{a}{a}$

\hspace{0.85cm} $= \ip{\sum^{N}_{i=1} \alpha_i \phi(x_i)}{\sum^{N}_{j=1} \alpha_j \phi(x_j)}$

\hspace{0.85cm} $= \sum^{N}_{i=1} \alpha_i \ip{\phi(x_i)}{\sum^{N}_{j=1} \alpha_j \phi(x_j)}$

\hspace{0.85cm} $= \sum^{N}_{i=1} \sum^{N}_{j=1} \alpha_i \alpha_j \ip{\phi(x_i)}{\phi(x_j)}$

\hspace{0.85cm} $= \sum^{N}_{i,j=1} \alpha_i \alpha_j K(x_i, x_j)$


\subsection{SVM with Kernel}

\hspace{0.45cm}
Given $X \subseteq \bbR^n, \, f : X \rightarrow \{-1, +1\}$, and a kernel $K : (\bbR^n \times \bbR^n) \rightarrow \bbR$.

Minimise $\sum^{N}_{i,j=1} \alpha_i \alpha_j K(x_i, x_j)$, where N is the number of total points.

Subject to: $(\sum^{N}_{i} \alpha_i K(x_i, x) - b)f(x) \geq 1$, $\forall x \in X$

\vspace{0.5cm}
Arguments for the \textbf{Key Obs.}:

\hspace{0.5cm} 1. Obvious if $\{ \phi(x) : x \in X \}$ span V (a $\in$ V)

\hspace{0.5cm} 2. Since the normal to the MMSHP is a difference between two points in the two hulls.

\hspace{0.5cm} 3. a is a linear combination of those embedded points which are nearest to the MMSHP.

\vspace{0.5cm}
$\rightarrow$ These vectors, whose embeddings are meant to build and define MMSHP are called "\textbf{Support Vectors}" of MMSHP. Hence the name, SVM.

\vspace{1.0cm}
\section{Intro. to Generalisation}

$\rightarrow$ We are given the sets of Good points (+1) and the Bad points (-1). By training on the existing data points, we try to find out a rule i.e. 'h'. Thus derived 'h' can be further used to classify and categorize future points as Good or otherwise Bad.

\begin{figure}[!h]
\centering
 \includegraphics[width=0.30\textwidth]{FDM_3.2.png}
 
 Fig 2.0 General Classification task 
 \end{figure}

$\rightarrow$ Hopefully, 'h' will separate unknown future good and bad points too.

\vspace{0.5cm}
$\rightarrow$ Important Questions:
\begin{itemize}
    \item Will the perfect separator for the test data separate real data reasonably well?
    \item Will your separator generalise?
    \item Did you algorithm gain any knowledge from the data?
    \item When and how well can we learn?
    \item What are the most lenient conditions under which generalisation happen?
    \item Can we have quantitative bounds on real world errors under these conditions?
\end{itemize}

\subsection{Supervised Learning Scenario}

$\rightarrow$ Let's say we have 10000 data points in X, and a function $f : X \rightarrow \{+1, -1\}$ in real world.

Now we are given just 100 points from X and is called \textbf{Training Sample Set or S}. We also are given the class of each data point i.e. f(x) for those x $\in$ S.

If we derive a separating hyperplane (a,b) which separates good and bad points in S i.e. $\forall x \in S, \, g(x) = sign(\ip{x}{a} - b)$.

We have to check whether the function g works equally well on points in X i.e. can we generalise g on X i.e. g(x) = f(x).

$\rightarrow$ Since few errors are acceptable,

\[ 
   "In \, Sample \, Error" \, or \, E_{in}(g) = |\{ x \in S : f(x) \neq g(x) \}| / |S|
\]
\[
    "Out \, of \, Sample \, Error" \, or \, E_{out}(g) = |\{ x \in X : f(x) \neq g(x) \}| / |X|
\]

\vspace{0.5cm}
$\rightarrow$ Is generalisation possible if:
\begin{itemize}
    \item \textbf{"Negation of Realisation Assumption" i.e. f on X is far from linearly separable?} - NO
    \item \textbf{"Sampling Bias" i.e. The 100 points of S is chosen by a malicious "teacher"?} - NO, because we might not even get proper set of points required for training. eg. selecting all 100 points to be good in S.
    \item \textbf{S is chosen by a good "teacher"?} - YES, but its difficult to happen. eg. we can choose points in S from convex hulls of G and B.
    \item \textbf{S is chosen uniformly at random from X i.e. Indifferent teacher?} - YES (to be discussed next.)
    \item \textbf{S is much smaller?} - NO, because size of S matters.
\end{itemize}

\vspace{0.5cm}
$\rightarrow$ \textbf{Assumptions for $\bbR^2$ case:} 
\begin{enumerate}
    \item $X \subseteq \bbR^2 (10000 \, points)$, $f : X \rightarrow \{-1,1\}$
    \item (X,f) is linearly separable by a line through origin, called \textbf{"Realisation Assumption"}.
    \item S is obtained by picking 100 points from X independently and uniformly at random(i.e. with replacement).
\end{enumerate}

\vspace{0.5cm}
$\rightarrow$ General Observation: X is separable $\implies$ S is separable.

$\rightarrow$ We run PLA to find a line defined by a normal 'a' $\in \bbR^2$. This line classifies S correctly.

\[
g(x) = sign(\ip{x}{a}), \, g : \bbR^2 \rightarrow \bbR
\]
\[
E_{out}(g) = |\{ x \in X : f(x) \neq g(x) \}| / |X| << \epsilon << 1
\]

$\rightarrow$ \textbf{Claim:} $E_{out}(g) \leq \epsilon$ with high probability.
\[
P(E_{out}(g) \leq \epsilon) \geq 1 - \delta \, \, \, (where \, \delta << 1)
\]

$\rightarrow$ \textbf{Analysis:} $P(E_{out}(g) > \epsilon)$

So $E_{out}(g) > \epsilon$ under two types of cases:

\begin{figure}[!h]
\centering
 \includegraphics[width=0.40\textwidth]{FDM_3.3.png}
 
 Fig 2.1 Case(a).
 \end{figure}

\begin{figure}[!h]
\centering
 \includegraphics[width=0.40\textwidth]{FDM_3.4.png}
 
 Fig 2.2 Case(b).
 \end{figure}

$\rightarrow$ The case (a) could have occurred probably because the points lying in shaded region are not there in S. And using prob., there are $\epsilon.N$ points of X in the shaded region.

$\rightarrow$ For case (a), 
\[
P(PLA \, outputs \, g) = the \, prob. \, that \, no \, point \, from \, the \, shaded \, region \, is \, chosen
\]
\[
= (1 - \epsilon)^{100}
\]

$\rightarrow$ Consider theta ($\theta$) to be the smallest angle so that the shaded region has $> \epsilon.N$ points.
\[
P(PLA \, outputs \, a \, line \, which \, is \, at \, an \, angle \geq \theta) \leq (1 - \epsilon)^{100}
\]

$\rightarrow$ From case (a) and (b), $E_{out}(g) > \epsilon$ only if PLA outputs a separating line which has $\geq \theta$ counterclockwise rotated or otherwise $\geq \theta'$ clockwise rotated from 'h'.

P($E_{out}(g) > \epsilon$) $\leq (1 - \epsilon)^{100} + (1 - \epsilon)^{100}$

\hspace{2.65cm}
$\leq 2(1 - \epsilon)^{100}$

\hspace{2.65cm}
$\leq 2e^{-100\epsilon} \leq \delta$

\end{document}
